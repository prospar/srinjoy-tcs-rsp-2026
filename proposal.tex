\documentclass[12pt,a4paper]{article}

\input{./references/preamble}
\input{./references/macros}

\usepackage[top=0.8in,left=0.8in,right=0.8in,bottom=0.8in]{geometry}

\begin{document}

\date{} %leave this blank

\begin{center}
    \Large{Efficient data structures for GPU under oversubscription} \\
    \vspace{1em}
    \normalsize{\textbf{Srinjoy Sarkar}} \\
    \normalsize{Department of Computer Science Engineering}\\
    \normalsize{IIT Kanpur, India}  \\
    \normalsize{Email:  
    srinjoys23@cse.iitk.ac.in}
    \vspace{1em}
\end{center}

    %\swarnendu{Try to avoid tabs in general.}

    \section{Introduction}
    The high compute capability and memory bandwidth of GPUs, coupled with energy efficiency, have spurred their usage across a diverse set of application domains including scientific computing simulations, big data and large-scale graph analytics, and computational biology~\cite{nvidia-applications}. Improved compute capabilities have led to a steady rise in the working set sizes of
    GPU-accelerated applications, which now often far exceed the GPU global memory capacity~\cite{dynamap,uvm-forest-isca25}.
    Hence, the traditional copy-then-execute (CTE) programming model is giving way to the newer unified virtual memory (UVM) paradigm, where the CPU virtual address space is also visible to GPU kernels. GPU vendors have introduced memory virtualization support through features such
    as unified virtual addressing, demand paging, and prefetching. UVM allows the host CPU and discrete GPU devices to share a unified virtual address space, enabling the GPU runtime to page memory in and out as needed~\cite{hum-ppopp-2020,dynamap}. Demand paging in UVM-capable GPUs facilitates
    transparent memory oversubscription, improving programmer productivity. The cost of servicing GPU page faults can be reduced by prefetching a chunk of nearby pages~\cite{cppe-ipdps-2020,uvmsmart-isca-2019,uvm-forest-isca25,hpe-tcad-2020}. Writing correct but scalable multithreaded GPU kernels is a difficult art~\cite{massively-parallel-processors,kernelbench-arxiv}. In addition to the algorithm and the compute capabilities of the GPU device, the scalability of the application depends on the building blocks of the kernel, \eg, GPU data structures. For example, the performance of the Pinterest application improved substantially by switching to GPU-accelerated hash tables~\cite{pinterest-perf-boost}. Therefore, the design of more complex structures such as lists and trees is an active area of research~\cite{gpu-gems2}. Much recent work has focused on designing efficient data structures (\eg, hash tables, skip lists, B-trees, and membership query filters) for use in device code~\cite{slabhash-ipdps-2020,dachash-sbacpad-2021,warpcore-hipc-2020,warpdrive,parallel-hashmap-nvidia,survey-hashing-tpds-2020,bght,dycuckoo,gph-pamd-2025,gfsl-pact-2017,lockfree-skiplist-ipdpsw-2019,skiplist-survey-2025,multiversion-btree,gpu-filters-ppopp23,harmonia-gpu,gpu-btree,gpu-gelhash,survey-hashing-tpds-2020,lockfree-ds-icpads-2012,quotient-filter,stdgpu,cucollections-nvidia,gpu-cds}.
    
    \section{The Problem}
    The steady increase in the working set size and irregular access patterns of GPU applications require more efficient memory management so that the GPU data structures efficiently scale with the input size. However, existing studies primarily focus on the traditional CTE model and do not address how to design efficient data structures that oversubscribe memory, and developers fall back to UVM. While UVM simplifies programming, its performance remains a significant concern due to frequent far faults
    \footnote{Far faults, which occur when a requested page is absent in GPU global memory, require multiple PCIe roundtrips, and are resolved after interacting with the CPU page tables. On \NVIDIA GPUs, a far fault from a CUDA thread stalls the progress of the warp, possibly affecting the overall GPU throughput.},
    high address translation overheads, and high volume of page replacement~\cite{suv-micro-2024,in-depth-analysis-uvm-sc21,fine-grain-quantitative-uvm-taco24,batch-aware-asplos-2020,uvmsmart-isca-2019,cppe-ipdps-2020,irregular-page-walks-isca-2018,heterogeneous-uvm,uvm-forest-isca25,dynamap,uvmbench-arxiv}. For instance, Lin et al.~\cite{oversubscription-dl-training} find UVM-based implementations competitive with the CTE model for workloads that fit in GPU memory, but about $7\times$ slower with oversubscription. The performance degradation with UVM is more prominent in applications with irregular access patterns and pointer chasing~\cite{fine-grain-quantitative-uvm-taco24,demystifying-uvm-cost-ipdps2021,in-depth-analysis-uvm-sc21}.
    
    Hash tables is a popular concurrent data structures for storing key-value pairs~\cite{art-of-multiprocessor-programming}.
    Hash tables are widely used in various scientific applications like metagenomic sequencing~\cite{metacache-icpp-2021}, photodna~\cite{photodna-ares-2023}, and database indexing workloads that are accelerated using the GPU but oversubscribe GPU memory.
    In metagenomic sequencing, a hash table is used as the genome database that is later used for query processing. The size of the entire dataset can grow to 50+~GB, easily overflowing the GPU capacity.

    Hash tables are characterized by input-dependent access locality~\cite{art-of-multiprocessor-programming}.
    Lack of locality in the input operation strings and sub-optimal implementation choices can lead to irregular access patterns and poor data locality. When memory oversubscription is
    coupled with these performance pathologies, these data structures exhibit very poor scalability.
    
    \section{Existing Solutions}
    No current solutions for such a problem hasn't been explored as far our knowledge. For most of such problems, the best way currently in usage is to execute the huge input workload in multiple batches such that each workload fits within the GPU memory. For certain cases, where a system has multiple GPUs ~\cite{warpdrive} proposed a multi gpu hashtable that can use both the GPUs and in turn can handle large workloads. But this type of setup is not only expensive but will also run into the same problem for even larger workloads.
    
    \section{Our current work}
    We have designed UVM-based implementation of hash tables with modified structures that performes well even with memory oversubscription. We compared our design with the existing state-of-the-art hashtables like cucollections\href{https://github.com/NVIDIA/cuCollections/tree/dev} and ~\cite{warpcore}. For valid comparison, we update this models to support memory oversubscription and we observe that they suffer greatly because of far faults. So we designed our hashtable such that these far faults are reduced.
    Our current work is currently under submission in a reputed conference.
    
    \section{Future work}
    We are planning on extending this idea to amq ie, designing a amq that doesn't suffer a huge performance overhead for memory oversubscription. We are
    also planning on improving the algorithms of the current state-of-the-art amqs such that they receive a substantial performance boost. 
    
   
    \bibliography{./references/venue-full,./references/references}

\end{document}