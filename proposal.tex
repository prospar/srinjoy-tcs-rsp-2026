\documentclass[12pt,a4paper]{article}

\input{./references/preamble}
\input{./references/macros}

\usepackage[top=0.8in,left=0.8in,right=0.8in,bottom=0.8in]{geometry}

\begin{document}

\date{} %leave this blank

\begin{center}
    \Large{Designing Efficient GPU Data Structures for Oversubscription} \\
    \vspace{1em}
    \normalsize{\textbf{Srinjoy Sarkar}} \\
    \normalsize{Department of Computer Science Engineering,}\\
    \normalsize{IIT Kanpur, India}  \\
    \normalsize{Email:  
    srinjoys23@cse.iitk.ac.in}
    \vspace{1em}
\end{center}

    %\swarnendu{Try to avoid tabs in general.}

\begin{abstract}
  Efficient concurrent data structures are important building blocks for accelerating applications on GPUs. With ever-increasing memory footprint of GPU workloads, data structures used by kernels can exceed global memory capacity. Using the unified virtual memory (UVM) model is a popular approach for kernels to oversubscribe GPU memory without the need for explicit memory management by a programmer. However, data structures executing with UVM can suffer from performance degradation due to the high overheads associated with data migration and thrashing under irregular access patterns.

  We propose to work on efficient GPU data structure designs to handle use cases
  where the application working set oversubscribes GPU memory. In particular, we
  plan to work on the following popular data structures: hash table, skip list,
  and approximate membership filter. We have profiled existing implementations
  of these data structures, and have identified several bottlenecks in the
  design. Furthermore, these data structures are characterized by random
  accesses, and hence encounter frequent pages faults and unnecessary data
  migrations between the CPU and the GPU with the UVM programming model. We plan
  to explore alternate designs (\eg, hierarchical layout) for these data
  structures to maximize access locality. The insight is that the outer-level
  containers enable efficient jumps to desired regions of the data structure,
  while the inner containers allows operating on the data. Our preliminary
  results show that our custom data structure designs substantially improve
  performance over optimized UVM baselines while supporting high degree of GPU
  memory oversubscription for many inputs.
\end{abstract}


    \section{Introduction}
    The high compute capability and memory bandwidth of GPUs, coupled with energy efficiency, have spurred their usage across a diverse set of application domains including scientific computing simulations, big data and large-scale graph analytics, and computational biology~\cite{nvidia-applications}. Improved compute capabilities have led to a steady rise in the working set sizes of
    GPU-accelerated applications, which now often far exceed the GPU global memory capacity~\cite{dynamap,uvm-forest-isca25}.
    Hence, the traditional copy-then-execute (CTE) programming model is giving way to the newer unified virtual memory (UVM) paradigm, where the CPU virtual address space is also visible to GPU kernels. GPU vendors have introduced memory virtualization support through features such
    as unified virtual addressing, demand paging, and prefetching. UVM allows the host CPU and discrete GPU devices to share a unified virtual address space, enabling the GPU runtime to page memory in and out as needed~\cite{hum-ppopp-2020,dynamap}. Demand paging in UVM-capable GPUs facilitates
    transparent memory oversubscription, improving programmer productivity. The cost of servicing GPU page faults can be reduced by prefetching a chunk of nearby pages~\cite{cppe-ipdps-2020,uvmsmart-isca-2019,uvm-forest-isca25,hpe-tcad-2020}. Writing correct but scalable multithreaded GPU kernels is a difficult art~\cite{massively-parallel-processors,kernelbench-arxiv}. In addition to the algorithm and the compute capabilities of the GPU device, the scalability of the application depends on the building blocks of the kernel, \eg, GPU data structures. For example, the performance of the Pinterest application improved substantially by switching to GPU-accelerated hash tables~\cite{pinterest-perf-boost}. Therefore, the design of more complex structures such as lists and trees is an active area of research~\cite{gpu-gems2}. Much recent work has focused on designing efficient data structures (\eg, hash tables, skip lists, B-trees, and membership query filters) for use in device code~\cite{slabhash-ipdps-2020,dachash-sbacpad-2021,warpcore-hipc-2020,warpdrive,parallel-hashmap-nvidia,survey-hashing-tpds-2020,bght,dycuckoo,gph-pamd-2025,gfsl-pact-2017,lockfree-skiplist-ipdpsw-2019,skiplist-survey-2025,multiversion-btree,gpu-filters-ppopp23,harmonia-gpu,gpu-btree,gpu-gelhash,survey-hashing-tpds-2020,lockfree-ds-icpads-2012,quotient-filter,stdgpu,cucollections-nvidia,gpu-cds}.

    
    While UVM simplifies programming, its efficiency remains a major concern due
    to frequent far faults and high address translation overheads. Memory
    virtualization on the GPU introduces additional sources of overhead (e.g.,
    address translation and page migration) that are opaque to the programmer.
    Demand paging in UVM is implemented by the GPU's UVM driver, which processes
    far faults\footnote{Far faults occur when a requested page is absent in
      device memory and is present in host memory.} from the CPU as well as GPU,
    and transparently migrates data as required. The GPU hardware and the driver
    may optionally implement additional features like prefetching to optimize
    data migration~\cite{uvmsmart-isca-2019,uvm-forest-isca25}, but these
      techniques often fail when applications with irregular access patterns
      oversubscribe memory.

    \section{The Problem}
    The steady increase in the working set size and irregular access patterns of GPU applications require more efficient memory management so that the GPU data structures efficiently scale with the input size. However, existing studies primarily focus on the traditional CTE model and do not address how to design efficient data structures that oversubscribe memory, and developers fall back to UVM. While UVM simplifies programming, its performance remains a significant concern due to frequent far faults
    \footnote{Far faults, which occur when a requested page is absent in GPU global memory, require multiple PCIe roundtrips, and are resolved after interacting with the CPU page tables. On \NVIDIA GPUs, a far fault from a CUDA thread stalls the progress of the warp, possibly affecting the overall GPU throughput.},
    high address translation overheads, and high volume of page replacement~\cite{suv-micro-2024,in-depth-analysis-uvm-sc21,fine-grain-quantitative-uvm-taco24,batch-aware-asplos-2020,uvmsmart-isca-2019,cppe-ipdps-2020,irregular-page-walks-isca-2018,heterogeneous-uvm,uvm-forest-isca25,dynamap,uvmbench-arxiv}. For instance, Lin et al.~\cite{oversubscription-dl-training} find UVM-based implementations competitive with the CTE model for workloads that fit in GPU memory, but about $7\times$ slower with oversubscription. The performance degradation with UVM is more prominent in applications with irregular access patterns and pointer chasing~\cite{fine-grain-quantitative-uvm-taco24,demystifying-uvm-cost-ipdps2021,in-depth-analysis-uvm-sc21}.
    
    Hash tables is a popular concurrent data structures for storing key-value pairs~\cite{art-of-multiprocessor-programming}.
    Hash tables are widely used in various scientific applications like metagenomic sequencing~\cite{metacache-icpp-2021}, photodna~\cite{photodna-ares-2023}, and database indexing workloads that are accelerated using the GPU but oversubscribe GPU memory.
    In metagenomic sequencing, a hash table is used as the genome database that is later used for query processing. The size of the entire dataset can grow to 50+~GB, easily overflowing the GPU capacity.

    Hash tables are characterized by input-dependent access locality~\cite{art-of-multiprocessor-programming}.
    Lack of locality in the input operation strings and sub-optimal implementation choices can lead to irregular access patterns and poor data locality. When memory oversubscription is
    coupled with these performance pathologies, these data structures exhibit very poor scalability.
    
    
    \section{Existing Solutions}
    No current solutions for such a problem hasn't been explored as far our knowledge. For most of such problems, the best way currently in usage is to execute the huge input workload in multiple batches such that each workload fits within the GPU memory. For certain cases, where a system has multiple GPUs ~\cite{warpdrive} proposed a multi gpu hashtable that can use both the GPUs and in turn can handle large workloads. But this type of setup is not only expensive but will also run into the same problem for even larger workloads.
    
    \section{Relevant GPU Hashtables}
    
    As mentioned hashtables are data structures that stores key-value pairs. Now when two different keys collide for the same empty slot, hashtable provides two types of methods to solve this collision, which in turn divides hashtables in two broader classes: open addressing hashtable and closed 
    addressing hashtable. 
    Based on this two method, as far as our knowledge the most two relevant works are ~\cite{slabhash-ipdps-2020} and ~\cite{warpcore}.
    Slabhash uses a closed addressing based collision resolution technique. It uses a Work Coperative Work Sharing(WCWS) strategy, where each warp gets a(a block of 32 threads working in lock step manner) gets each operation. They uses specialized nodes(called slabnodes) that stores 31 key-value pairs and a pointer to next node. They uses a special memory allocator called slab allocator, which allocates a new slabnode whenever necessary.
    \begin{figure}
        \centering
          \includegraphics[scale=0.7]{"./figs/slabhash.drawio.pdf"}
        \caption{Slabhash layout}
        \label{fig:slabhash-design}
    \end{figure}
  
    However, closed-addressing hash tables suffer from frequent random memory accesses. This behavior significantly degrades cache locality and renders hardware prefetching largely ineffective for large workloads. In particular, a key that would logically be placed in an adjacent slot may instead require the allocation of a new node located in a distant memory region, further exacerbating memory latency and performance variability.
  
    Accordingly, we adopt WarpCore as the baseline for our design, which employs open addressing as its collision-resolution strategy. Open addressing offers superior cache locality, making it particularly well suited to our approach, as hardware prefetchers can be leveraged effectively. WarpCore also employs the Warp-Cooperative Work Sharing (WCWS) strategy: if a thread within a warp fails to insert a key, another thread in the same warp that has identified an empty slot attempts the insertion. If all threads in the warp fail, each thread computes a secondary hash and collectively probes the next consecutive 32 slots in the hash table. 
    
    Both the hashtable uses cooperative groups(cg) which is similar to a warp except now the warps becomes smaller with 2,4,8 or 16 threads(as specified in the program).
  
    \begin{figure}
    \centering
      \includegraphics[scale=0.7]{"./figs/WCWS.pdf"}
    \caption{Warpcore layout}
    \label{fig:warpcore-design}
    \end{figure}
    
    But all of this works develop in memory hashtable.
     
    
    \section{Our current work}
    We have designed UVM-based implementation of hash tables with modified structures that performes well even with memory oversubscription. We compared our design with the existing state-of-the-art hashtables like cucollections\href{https://github.com/NVIDIA/cuCollections/tree/dev} and ~\cite{warpcore}. For valid comparison, we update this models to support memory oversubscription and we observe that they suffer greatly because of far faults. So we designed our hashtable such that these far faults are reduced.
    Our current work is currently under submission in a reputed conference.
    
    \section{Future work}
    We are planning on extending this idea to amq ie, designing a amq that doesn't suffer a huge performance overhead for memory oversubscription. We are
    also planning on improving the algorithms of the current state-of-the-art amqs such that they receive a substantial performance boost. 
    
\begin{small}
    \bibliographystyle{plain}
    \bibliography{./references}
  \end{small}
  
\end{document}
